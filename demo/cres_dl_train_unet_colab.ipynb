{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5xBV8mSKdFBg",
        "d_mCF1VKdEu1",
        "Hufce6HUR15P",
        "9DIfGCUHrsuT",
        "dlVcFEdM_Lrj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CRES Deep Learning: \n",
        "## --- *Image Segmentation with UNET* --- \n",
        "\n",
        "-----\n",
        "**Overview**\n",
        "\n",
        "* Below is an example of how to load a simulated dataset and train an image segmentation model on it.\n",
        "\n",
        "-----\n",
        "**Instructions**\n",
        "\n",
        "* Start by uploading a .zip file with the .spec file dataset. This may take a second. \n",
        "    0. Start by following the directions on the [README](https://github.com/Helium6CRES/he6-cres-deep-learning) to make a training dataset. Compress the root directory that contains both the spec files and labels.\n",
        "    1. Click on files on right menu bar. \n",
        "    2. Click upload. \n",
        "    3. Upload a .zip containing data files. \n",
        "    4. Need to have instructions on the readme for how to create these files. \n",
        "* Then follow the cells below to visualize the dataset and train the lightning module. \n",
        "\n",
        "-----\n",
        "**Tips**\n",
        "\n",
        "* If you restart the runtime you don't lose all your imported data but if you restart and delete then you do. \n",
        "* If you change the runtime type (GPU to CPU for example) you lose all imported data. \n",
        "\n",
        "-----\n",
        "**Resources**\n",
        "\n",
        "* [Pytorch docs](https://pytorch.org/docs/stable/index.html)\n",
        "* [Torchvision docs](https://pytorch.org/vision/stable/index.html)\n",
        "* [Useful for uploading to colab](https://medium.com/@vishakha1203/easiest-way-to-upload-large-datasets-to-google-colab-1f89231844dc)\n",
        "\n",
        "-----\n",
        "**Project Links**: \n",
        "* [he6-cres-deep-learning github page](https://github.com/Helium6CRES/he6-cres-deep-learning)"
      ],
      "metadata": {
        "id": "-qAJ7tvnVtGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.&nbsp;Imports"
      ],
      "metadata": {
        "id": "Jq8zlPfK3-Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install torch == 1.12.1\n",
        "! pip install torchvision == 0.13.1\n",
        "! pip install pytorch-lightning == 1.6.4\n",
        "! pip install pytorch-lightning-bolts\n",
        "! pip install torchmetrics == 0.9.1\n",
        "! pip install matplotlib == 3.1.3\n",
        "! pip install numpy == 1.21.6\n",
        "! pip install ipywidgets==7.7.0\n",
        "! pip install re==2.2.1"
      ],
      "metadata": {
        "id": "CPViS3XTH0pF"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep learning imports.\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks, make_grid\n",
        "from torchvision.ops import masks_to_boxes\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "import torchmetrics\n",
        "\n",
        "# Standard imports. \n",
        "from typing import List, Union\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# Necessary for creating our images.\n",
        "from skimage.draw import line_aa\n",
        "\n",
        "# Interactive widgets for data viz.\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual"
      ],
      "metadata": {
        "id": "n0H8m0yZHu0c"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Figurign out max pool: \n",
        "\n",
        "img = torch.rand((3,256,256))\n",
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upn5aJ7wy0LH",
        "outputId": "d4feb2e5-72d8-4b0f-f386-19cd02f8885c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxpool = nn.MaxPool2d(3, 3, return_indices=False)\n",
        "imgs_mp = maxpool(img.float())\n",
        "imgs_mp.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVDjsvrTzEJl",
        "outputId": "ebbb88bf-63a2-413f-899b-534714ea22b3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 85, 85])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**import deep learning package from Helium6CRES organization github page**"
      ],
      "metadata": {
        "id": "3tlsUIQL4LWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/he6-cres-deep-learning\n",
        "!git clone https://github.com/Helium6CRES/he6-cres-deep-learning.git\n",
        "sys.path.append('/content/he6-cres-deep-learning')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qlCYPHJuwOt",
        "outputId": "0a549d7f-b9f6-457e-a321-e848622d8dee"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'he6-cres-deep-learning'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 92 (delta 49), reused 67 (delta 24), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (92/92), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIe20y_iaowo",
        "outputId": "61b0abf3-bef9-418b-f215-ab4d806936b0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%autoreload 2\n",
        "from he6_cres_deep_learning.deep_learning import ds\n",
        "from he6_cres_deep_learning.deep_learning import util \n",
        "from he6_cres_deep_learning.deep_learning import model "
      ],
      "metadata": {
        "id": "r8kL8ND9vetT"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.&nbsp;Load Data Dir to Colab"
      ],
      "metadata": {
        "id": "-bOKmRLQc64U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**load data from local**\n",
        "* Put path to the uploaded zip below.  "
      ],
      "metadata": {
        "id": "drPlxQHyPLvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/base_daq_config_snr.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "986vHhSyYVdu",
        "outputId": "d3fedba1-830e-44ef-dffd-b81b294720fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/base_daq_config_snr.zip\n",
            "replace base_daq_config_snr/label_files/test_label_0.spec? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.&nbsp;Visualize Dataset"
      ],
      "metadata": {
        "id": "kOuBRcAGcHo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/base_daq_config_snr\"\n",
        "cres_dm = ds.CRES_DM(root_dir = dataset_path, max_pool = 8, file_max = 12, batch_size= 4)"
      ],
      "metadata": {
        "id": "JnNN1fw5scD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "@interact\n",
        "def vizualize_label_targets(display_num_imgs= widgets.IntSlider(style= style,value=4,min=0,max=8,step=1, description = \"display_num_imgs\"),\n",
        "                            mask_alpha= widgets.FloatSlider(style= style,value=.5,min=0,max=1,step=.01, description = \"mask alpha\"),\n",
        "                            display_size = widgets.IntSlider(style= style, value=15,min=5,max=50,step=1),\n",
        "                            show_labels = widgets.Checkbox(style= style,value=True,description='target masks'),   \n",
        "                            ): \n",
        "\n",
        "\n",
        "    dataiter = iter(cres_dm.train_dataloader())\n",
        "\n",
        "    imgs, labels = dataiter.next()\n",
        "    print(imgs.shape)\n",
        "    print(imgs.dtype)\n",
        "\n",
        "    imgs = 255 - imgs.repeat(1, 3, 1, 1)\n",
        "    imgs = imgs[:display_num_imgs]\n",
        "    labels = labels[:display_num_imgs]\n",
        "    masks = util.labels_to_masks(labels)\n",
        "\n",
        "    result_images = [imgs[i] for i in range(display_num_imgs)]\n",
        "\n",
        "    if show_labels: \n",
        "        result_images = util.display_masks_unet(imgs, masks, cres_dm.class_map, alpha = mask_alpha)\n",
        "\n",
        "    grid = make_grid(result_images)\n",
        "    util.show(grid, figsize = (display_size, display_size))"
      ],
      "metadata": {
        "id": "b4_q1vEPqFZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.&nbsp;Train the Lightning Module\n",
        "\n",
        "* Too many spec/label files can overpower the ram. Start with `file_max` = 10, `max_pool` = 16 below. \n",
        "* If you have more classes you will need to change the `weights` tensor to have more values and the `num_classes` to match `max(labels)`. "
      ],
      "metadata": {
        "id": "-xFlNqGehn5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/base_daq_config_snr\"\n",
        "cres_dm = ds.CRES_DM(root_dir = dataset_path, max_pool = 8, file_max = 50, batch_size = 1)\n",
        "\n",
        "# Define weights for loss function. \n",
        "weights = torch.tensor([1,10]).float()\n",
        "\n",
        "# Create callback for ModelCheckpoints. \n",
        "checkpoint_callback = ModelCheckpoint(filename='{epoch:02d}', save_top_k = 50, monitor = \"Loss/val_loss\", every_n_epochs = 1)\n",
        "\n",
        "# Define Logger. \n",
        "logger = TensorBoardLogger(\"tb_logs\", name=\"cres_image_segmentation\", log_graph = True)\n",
        "\n",
        "# Create Instance of Lightning Module. \n",
        "img_seg_lm = model.LightningImageSegmentation(in_channels=1, \n",
        "                                        num_classes=2, \n",
        "                                        first_feature_num = 4, \n",
        "                                        num_layers = 2, \n",
        "                                        skip_connect = True, \n",
        "                                        kernel_size = 3, \n",
        "                                        bias = False, \n",
        "                                        weight_loss = weights)\n",
        "\n",
        "# -----------Set device.------------------\n",
        "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create an instance of a Trainer.\n",
        "trainer = pl.Trainer(logger = logger, callbacks = [checkpoint_callback], accelerator = device, max_epochs = 20, log_every_n_steps = 2)\n",
        "\n",
        "# Fit. \n",
        "trainer.fit(img_seg_lm, cres_dm)"
      ],
      "metadata": {
        "id": "S_xx4gR08rnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tensorboard**"
      ],
      "metadata": {
        "id": "d8YgICfqpp5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs/"
      ],
      "metadata": {
        "id": "M2AqWA-CjuDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.&nbsp; Visualize Predictions\n"
      ],
      "metadata": {
        "id": "frtldUbWicMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**get test set**"
      ],
      "metadata": {
        "id": "p3R1KOfXicML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataiter = iter(cres_dm.test_dataloader())"
      ],
      "metadata": {
        "id": "jH3Cb6pPicMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**execute following cell again to see more test data**"
      ],
      "metadata": {
        "id": "PpJv63TDicMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, labels = test_dataiter.next()\n",
        "masks = util.labels_to_masks(labels)"
      ],
      "metadata": {
        "id": "-gjIvoYJicMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**visualize predictions on test set**"
      ],
      "metadata": {
        "id": "rTmPOYXVicMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "version = 5\n",
        "\n",
        "@interact\n",
        "def vizualize_labels_preds( epoch = widgets.IntSlider(value=19,min=0,max=19,step=1),\n",
        "                            show_labels = widgets.Checkbox(value=False,description='display_labels'),\n",
        "                            show_preds = widgets.Checkbox(value=False,description='display_preds'),\n",
        "                            display_size = widgets.IntSlider(value=10,min=2,max=50,step=1), \n",
        "                            mask_threshold = widgets.FloatSlider(value=.5,min=0,max=1,step=.0001,description='mask_thresh'),\n",
        "                        ): \n",
        "\n",
        "  \n",
        "    PATH = '/content/tb_logs/cres_image_segmentation/version_{}/checkpoints/epoch={:02d}.ckpt'.format(version, epoch)\n",
        "  \n",
        "    loaded_lm = model.LightningImageSegmentation.load_from_checkpoint(PATH)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = loaded_lm(imgs)\n",
        "\n",
        "    probs = logits.softmax(dim = 1)\n",
        "\n",
        "    imgs_display = 255 - imgs.repeat(1, 3, 1, 1)\n",
        "    result_image = [imgs_display[i] for i in range(len(imgs_display))]\n",
        "\n",
        "    if  show_labels: \n",
        "        result_image = util.display_masks_unet(result_image, masks, cres_dm.class_map)\n",
        "    if  show_preds: \n",
        "        preds = (probs > mask_threshold)\n",
        "        result_image = util.display_masks_unet(result_image, preds, cres_dm.class_map)\n",
        "    grid = make_grid(result_image)\n",
        "    util.show(grid, figsize = (display_size,display_size))"
      ],
      "metadata": {
        "id": "fwOIli8KicMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1tGWof3D4e9k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}