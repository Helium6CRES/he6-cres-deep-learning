{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qAJ7tvnVtGo"
   },
   "source": [
    "# CRES Deep Learning: \n",
    "## --- *Image Segmentation with UNET* --- \n",
    "\n",
    "-----\n",
    "**Overview**\n",
    "\n",
    "* Below is an example of how to load a simulated dataset and train an image segmentation model on it.\n",
    "\n",
    "-----\n",
    "**Instructions**\n",
    "\n",
    "* Start by uploading a .zip file with the .spec file dataset. This may take a second. \n",
    "    0. Start by following the directions on the [README](https://github.com/Helium6CRES/he6-cres-deep-learning) to make a training dataset. Compress the root directory that contains both the spec files and labels.\n",
    "    1. Click on files on right menu bar. \n",
    "    2. Click upload. \n",
    "    3. Upload a .zip containing data files. \n",
    "    4. Need to have instructions on the readme for how to create these files. \n",
    "* Then follow the cells below to visualize the dataset and train the lightning module. \n",
    "\n",
    "-----\n",
    "**Tips**\n",
    "\n",
    "* If you restart the runtime you don't lose all your imported data but if you restart and delete then you do. \n",
    "* If you change the runtime type (GPU to CPU for example) you lose all imported data. \n",
    "\n",
    "-----\n",
    "**Resources**\n",
    "\n",
    "* [Pytorch docs](https://pytorch.org/docs/stable/index.html)\n",
    "* [Torchvision docs](https://pytorch.org/vision/stable/index.html)\n",
    "* [Useful for uploading to colab](https://medium.com/@vishakha1203/easiest-way-to-upload-large-datasets-to-google-colab-1f89231844dc)\n",
    "\n",
    "-----\n",
    "**Project Links**: \n",
    "* [he6-cres-deep-learning github page](https://github.com/Helium6CRES/he6-cres-deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jq8zlPfK3-Vn"
   },
   "source": [
    "## 1.&nbsp;Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "CPViS3XTH0pF"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install torch == 1.12.1\n",
    "! pip install torchvision == 0.13.1\n",
    "! pip install pytorch-lightning == 1.6.4\n",
    "! pip install pytorch-lightning-bolts\n",
    "! pip install torchmetrics == 0.9.1\n",
    "! pip install matplotlib == 3.1.3\n",
    "! pip install numpy == 1.21.6\n",
    "! pip install ipywidgets==7.7.0\n",
    "! pip install re==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n0H8m0yZHu0c"
   },
   "outputs": [],
   "source": [
    "# Deep learning imports.\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks, make_grid\n",
    "from torchvision.ops import masks_to_boxes\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "# Standard imports. \n",
    "from typing import List, Union\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Necessary for creating our images.\n",
    "from skimage.draw import line_aa\n",
    "\n",
    "# Interactive widgets for data viz.\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tlsUIQL4LWV"
   },
   "source": [
    "**import deep learning package from Helium6CRES organization github page**\n",
    "\n",
    "* May need to git clone if you have not already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIe20y_iaowo",
    "outputId": "61b0abf3-bef9-418b-f215-ab4d806936b0"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r8kL8ND9vetT"
   },
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "sys.path.append('/home/drew/He6CRES/he6-cres-deep-learning/')\n",
    "from he6_cres_deep_learning.deep_learning import ds\n",
    "from he6_cres_deep_learning.deep_learning import util \n",
    "from he6_cres_deep_learning.deep_learning import model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOuBRcAGcHo3"
   },
   "source": [
    "## 3.&nbsp;Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JnNN1fw5scD0"
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/media/drew/T7 Shield/cres_deep_learning/training_data/config/simple_ds\"\n",
    "cres_dm = ds.CRES_DM(root_dir = dataset_path, max_pool = 16, file_max = 12, batch_size= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "b4_q1vEPqFZ0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9490e6c0a1432fafe703fe20b7a153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4, description='display_num_imgs', max=8, style=SliderStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "@interact\n",
    "def vizualize_label_targets(display_num_imgs= widgets.IntSlider(style= style,value=4,min=0,max=8,step=1, description = \"display_num_imgs\"),\n",
    "                            mask_alpha= widgets.FloatSlider(style= style,value=.5,min=0,max=1,step=.01, description = \"mask alpha\"),\n",
    "                            display_size = widgets.IntSlider(style= style, value=15,min=5,max=50,step=1),\n",
    "                            show_labels = widgets.Checkbox(style= style,value=True,description='target masks'),   \n",
    "                            ): \n",
    "\n",
    "\n",
    "    dataiter = iter(cres_dm.train_dataloader())\n",
    "\n",
    "    imgs, labels = dataiter.next()\n",
    "\n",
    "    imgs = 255 - imgs.repeat(1, 3, 1, 1)\n",
    "    imgs = imgs[:display_num_imgs]\n",
    "    labels = labels[:display_num_imgs]\n",
    "    masks = util.labels_to_masks(labels)\n",
    "\n",
    "    result_images = [imgs[i] for i in range(display_num_imgs)]\n",
    "\n",
    "    if show_labels: \n",
    "        result_images = util.display_masks_unet(imgs, masks, cres_dm.class_map, alpha = mask_alpha)\n",
    "\n",
    "    grid = make_grid(result_images)\n",
    "    util.show(grid, figsize = (display_size, display_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xFlNqGehn5k"
   },
   "source": [
    "## 4.&nbsp;Train the Lightning Module\n",
    "\n",
    "* Too many spec/label files can overpower the ram. Start with `file_max` = 10, `max_pool` = 16 below. \n",
    "* If you have more classes you will need to change the `weights` tensor to have more values and the `num_classes` to match `max(labels)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "S_xx4gR08rnZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tb_logs/cres_image_segmentation\n",
      "\n",
      "  | Name      | Type             | Params | In sizes         | Out sizes       \n",
      "-------------------------------------------------------------------------------------\n",
      "0 | train_acc | Accuracy         | 0      | ?                | ?               \n",
      "1 | train_f1  | F1Score          | 0      | ?                | ?               \n",
      "2 | val_acc   | Accuracy         | 0      | ?                | ?               \n",
      "3 | val_f1    | F1Score          | 0      | ?                | ?               \n",
      "4 | loss      | CrossEntropyLoss | 0      | ?                | ?               \n",
      "5 | model     | UNET             | 30.5 K | [4, 1, 256, 256] | [4, 2, 256, 256]\n",
      "-------------------------------------------------------------------------------------\n",
      "30.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "30.5 K    Total params\n",
      "0.122     Total estimated model params size (MB)\n",
      "2022-11-22 17:42:03.707809: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/drew/Products/root/lib/:/home/drew/Products/root/lib\n",
      "2022-11-22 17:42:03.708165: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/drew/He6CRES/he6-cres-deep-learning/he6_cres_deep_learning/deep_learning/model.py:206: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.shape != skip_connection.shape:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drew/home/drew/Products/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n",
      "/home/drew/home/drew/Products/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:245: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  category=PossibleUserWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3326f56e9e814be5b9d2eb6291dd4338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = \"/media/drew/T7 Shield/cres_deep_learning/training_data/config/simple_ds\"\n",
    "cres_dm = ds.CRES_DM(root_dir = dataset_path, max_pool = 16, file_max = 50, batch_size = 1)\n",
    "\n",
    "# Define weights for loss function. \n",
    "weights = torch.tensor([1,10]).float()\n",
    "\n",
    "# Create callback for ModelCheckpoints. \n",
    "checkpoint_callback = ModelCheckpoint(filename='{epoch:02d}', save_top_k = 50, monitor = \"Loss/val_loss\", every_n_epochs = 1)\n",
    "\n",
    "# Define Logger. \n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"cres_image_segmentation\", log_graph = True)\n",
    "\n",
    "# Create Instance of Lightning Module. \n",
    "img_seg_lm = model.LightningImageSegmentation(in_channels=1, \n",
    "                                        num_classes=2, \n",
    "                                        first_feature_num = 4, \n",
    "                                        num_layers = 2, \n",
    "                                        skip_connect = True, \n",
    "                                        kernel_size = 3, \n",
    "                                        bias = False, \n",
    "                                        weight_loss = weights)\n",
    "\n",
    "# -----------Set device.------------------\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create an instance of a Trainer.\n",
    "trainer = pl.Trainer(logger = logger, callbacks = [checkpoint_callback], accelerator = device, max_epochs = 10, log_every_n_steps = 2)\n",
    "\n",
    "# Fit. \n",
    "trainer.fit(img_seg_lm, cres_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8YgICfqpp5G"
   },
   "source": [
    "**tensorboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "M2AqWA-CjuDW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1b6f90beadf5b85b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1b6f90beadf5b85b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frtldUbWicMI"
   },
   "source": [
    "## 5.&nbsp; Visualize Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3R1KOfXicML"
   },
   "source": [
    "**get test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jH3Cb6pPicMM"
   },
   "outputs": [],
   "source": [
    "test_dataiter = iter(cres_dm.test_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpJv63TDicMN"
   },
   "source": [
    "**execute following cell again to see more test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-gjIvoYJicMP"
   },
   "outputs": [],
   "source": [
    "imgs, labels = test_dataiter.next()\n",
    "masks = util.labels_to_masks(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTmPOYXVicMQ"
   },
   "source": [
    "**visualize predictions on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fwOIli8KicMS"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a8f8d7e88b4e5ab002965050c6252f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=9, description='epoch', max=19), Checkbox(value=False, description='dispâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "version = 0\n",
    "\n",
    "@interact\n",
    "def vizualize_labels_preds( epoch = widgets.IntSlider(value=9,min=0,max=19,step=1),\n",
    "                            show_labels = widgets.Checkbox(value=False,description='display_labels'),\n",
    "                            show_preds = widgets.Checkbox(value=False,description='display_preds'),\n",
    "                            display_size = widgets.IntSlider(value=10,min=2,max=50,step=1), \n",
    "                            mask_threshold = widgets.FloatSlider(value=.5,min=0,max=1,step=.0001,description='mask_thresh'),\n",
    "                        ): \n",
    "\n",
    "  \n",
    "    PATH = '/home/drew/He6CRES/he6-cres-deep-learning/demo/tb_logs/cres_image_segmentation/version_{}/checkpoints/epoch={:02d}.ckpt'.format(version, epoch)\n",
    "  \n",
    "    loaded_lm = model.LightningImageSegmentation.load_from_checkpoint(PATH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = loaded_lm(imgs)\n",
    "\n",
    "    probs = logits.softmax(dim = 1)\n",
    "\n",
    "    imgs_display = 255 - imgs.repeat(1, 3, 1, 1)\n",
    "    result_image = [imgs_display[i] for i in range(len(imgs_display))]\n",
    "\n",
    "    if  show_labels: \n",
    "        result_image = util.display_masks_unet(result_image, masks, cres_dm.class_map)\n",
    "    if  show_preds: \n",
    "        preds = (probs > mask_threshold)\n",
    "        result_image = util.display_masks_unet(result_image, preds, cres_dm.class_map)\n",
    "    grid = make_grid(result_image)\n",
    "    util.show(grid, figsize = (display_size,display_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tGWof3D4e9k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5xBV8mSKdFBg",
    "d_mCF1VKdEu1",
    "Hufce6HUR15P",
    "9DIfGCUHrsuT",
    "dlVcFEdM_Lrj"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
